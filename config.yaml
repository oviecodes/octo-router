providers:
  - name: openai
    apiKey: ${OPENAI_API_KEY}
    enabled: true
  
  - name: anthropic
    apiKey: ${ANTHROPIC_API_KEY}
    enabled: true
  
  - name: gemini
    apiKey: ${GEMINI_API_KEY}
    enabled: true  # Can disable providers

routing:
  strategy: "weighted" # one of "cost-based", "round-robin", "weighted", "latency-based"

  # Cost-based routing options
  costOptions:
    defaultTier: "premium"      # Default tier when not specified in request (optional)
    minimumTier: ""              # Minimum tier to use - never go below this (optional)
    tierStrategy: "same-tier"    # "same-tier", "allow-downgrade", "cheapest"

  # Weights for weighted routing (provider name: weight)
  # keys must match provider names (lowercase)
  weights:
    gemini: 10
    openai: 70
    anthropic: 20

  # Pipeline Policies (The "Bouncer" Layer)
  policies:
    semantic:
      enabled: true
      engine: "embedding"  # Engines: "keyword" or "embedding"
      threshold: 0.45      
      model_path: "assets/models/embedding.onnx"
      shared_lib_path: "/opt/homebrew/lib/libonnxruntime.dylib" # Optional: Explicit path
      default_group: "general"
      extend_default: true # Enable system-default intents (coding, fast-chat)
      groups:
        - name: "coding"
          use_system_default: true # Inherit examples from system defaults
          required_capability: "coding" 
        - name: "fast-chat"
          intent_description: "Simple conversational interactions and basic knowledge." # use intent_keyword for keyword engine
          examples:
            - "hello! how is your day going?"
            - "tell me a joke about robots"
          allow_providers: ["gemini"]

  # Fallback chain
  fallbacks:
    - openai
    - anthropic
    - gemini

models:
  defaults:
    openai:
      model: "openai/gpt-4o-mini"
      maxTokens: 4096

    anthropic:
      model: "anthropic/claude-haiku-3"
      maxTokens: 4096

    gemini:
      model: "gemini/gemini-2.5-flash-lite"
      maxTokens: 4096

  catalog:
    # Standard models are built-in (OpenAI, Anthropic, Gemini).
    # Use this section to ADD new models or OVERRIDE existing ones.
    
    # Example: Override cost for GPT-4o
    # - id: "openai/gpt-4o"
    #   provider: "openai"
    #   name: "GPT-4o (Special Rate)"
    #   inputCost: 2.00
    #   outputCost: 8.00
    #   contextWindow: 128000
    #   tier: "premium"

    # Example: Add a local Ollama model (Future proofing)
    # - id: "ollama/llama3"
    #   provider: "ollama"  # Requires generic provider implementation
    #   name: "Llama 3 Local"
    #   inputCost: 0
    #   outputCost: 0
    #   contextWindow: 8192
    #   tier: "budget"

  # Route specific prompts to specific models
  rules:
    - if: prompt.length > 5000
      use: "anthropic/claude-sonnet-4"  # Better at long context

    - if: task == "code-generation"
      use: "openai/gpt-4o"  # Better at code

    - if: priority == "high"
      use: "openai/gpt-4o"  # Fastest, most capable
      provider: "openai"

cache:
  enabled: true
  ttl: 3600  # Cache for 1 hour
  
  # Semantic caching (advanced)
  semantic:
    enabled: false # Start with exact match
    similarityThreshold: 0.95  # 95% similar = cache hit
  
  # What to cache
  rules:
    - cache: true
      maxSize: 1024  # Max tokens to cache
    - cache: true
      if: request.temperature == 0  # Deterministic requests
    - cache: false
      if: request.user == "premium"

redis:
  addr: "localhost:6379"
  password: ""
  db: 0

security:
  apiKeys:
    - "sk-octo-dev-123"
    - "${ROUTER_API_KEY}"

limits:
  # Per-user rate limits
  requestsPerMinute: 100
  requestsPerDay: 10000
  
  # Budget controls
  dailyBudget: 50.00  # Stop after $50/day
  alertThreshold: 40.00  # Alert at $40
  
  # Per-provider limits (respect their rate limits)
  providers:
    openai:
      requestsPerMinute: 60
      budget: 0.00001
    anthropic:
      requestsPerMinute: 50
      budget: 0.0000705
    gemini:
      requestsPerMinute: 100
      budget: 1.00

resilience:
  timeout: 30000  # 30 second timeout
  
  retries:
    maxAttempts: 3
    initialDelay: 1000  # 1 second
    maxDelay: 10000     # 10 seconds
    backoffMultiplier: 2  # Exponential backoff
  
  circuitBreaker:
    failureThreshold: 5  # Open after 5 failures
    resetTimeout: 60000

costManagement:
  # Automatically switch to cheaper models when approaching budget
  autoDowngrade:
    enabled: true
    threshold: 0.9  # At 90% of budget, use cheaper models
    
  # Warn users about expensive requests
  warnExpensiveRequests:
    enabled: true
    threshold: 0.10  # Warn if single request > $0.10

# experiments:
#   - name: "claude-vs-gpt4"
#     enabled: true
#     traffic: 0.1  # 10% of requests
#     variants:
#       control: {provider: "openai", model: "openai/gpt-4o"}
#       treatment: {provider: "anthropic", model: "anthropic/claude-sonnet-4"}
  